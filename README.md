# Sales FAQ Bot

![python-image] ![fastapi-image] ![Qdrant-image] ![Gemini-image] ![HTML-image] ![HuggingFace-image]

> An intelligent Q&A system built on RAG (Retrieval-Augmented Generation) architecture, designed to provide fast and accurate answers to sales' FAQ. The system integrates vector retrieval, semantic search, and generative AI, offering a user-friendly web interface for real-time queries.

![5kome-utkoy](https://github.com/user-attachments/assets/73d561b0-fa34-4160-9587-0cc17f15a4db)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Prerequisites](#-prerequisites)
- [Getting Started](#-getting-started)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [Development](#-development)
- [Testing](#-testing)
- [Contact](#-contact)

---

## ğŸ¯ Overview

Sales FAQ Bot is an POC Q&A system designed to solve various questions that sales encounter in their daily work. Through advanced RAG technology, the system can:

- **Fast Retrieval**: Accurately find relevant documents from the knowledge base
- **Semantic Understanding**: Use LLM models to understand user query intent
- **Intelligent Answers**: Combine retrieval results with LLM to generate natural and fluent responses
- **Real-time Interaction**: Provide a demo web chat interface

The system uses FastAPI as the backend framework, Qdrant as the vector database, and integrates BGE-M3 embedding model with Google Gemini API to achieve high-performance semantic search and intelligent Q&A.

---

## ğŸ“¦ Prerequisites

Ensure your development environment meets the following requirements:

| Requirement | Version | Description |
|---------|------|------|
| Python | 3.12+ | Core runtime environment |
| uv | 0.9.17+ | Lightweight package manager for environment and dependency building |

### API Keys

The system requires the following API Keys (configure in `.env` file):

- `GEMINI_API_KEY` Google Gemini API key (for Gemini API)
- `AZURE_OPENAI_API_KEY` Azure OpenAI API key (for Azure OpenAI)
- `AZURE_OPENAI_ENDPOINT` Azure OpenAI endpoint URL
- `AZURE_OPENAI_API_VERSION` Azure OpenAI API version (optional, defaults to 2024-12-01-preview)
- `HUGGINGFACE_LLM_Model` HuggingFace access token (if using private models)

---

## ğŸš€ Getting Started

### 1. Clone the Project

```bash
# Clone the project (or download the project archive)
git clone https://github.com/xinhuang0716/salesFAQBot.git
cd salesFAQBot
```

### 2. Configure Environment Variables

Create a `.env` file in the project root directory:

```bash
# .env
# For Gemini API
GEMINI_API_KEY=your_gemini_api_key_here

# For Azure OpenAI API
AZURE_OPENAI_API_KEY=your_azure_openai_key_here
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint_here
AZURE_OPENAI_API_VERSION=2024-12-01-preview

# For HuggingFace (if using private models)
HUGGINGFACE_LLM_Model=your_huggingface_token_here
```

### 3. Prepare Knowledge Base

Place your FAQ knowledge base Excel file in the `knowledgeDoc/` folder.

**File Format Requirements:**
- File type: `.xlsx`
- Required columns: `id`, `source`, `topic`, `subtype`, `relevance`

Example:
| id | source | topic | subtype | relevance |
|----|--------|-------|---------|-----------|
| 1  | Internal Document | Account Opening Process | Online Account Opening | How to apply for online account opening? |

### 4. Configure System Parameters

Edit `config/config.yaml` to adjust system settings (optional):

```yaml
embedder:
  repo: "BAAI/bge-m3"  # Can switch to other embedding models

retriever:
  top_k: 3              # Number of documents to retrieve
  score_threshold: 0.5  # Similarity threshold
```

### 5. Start the Service

By default, the virtual ennvironment and dependencies will be automatically created when starting the server for the first time.

```powershell
# Start FastAPI server using uv
uv run main.py
```

After successful startup, you will see:
```
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8010
```

### 6. Access the Application

Open your browser and visit:
```
http://localhost:8010
```

---

## ğŸ’» Usage

### Web Interface Usage

1. **Open Application**: Visit `http://localhost:8010` in your browser
2. **Enter Question**: Type your question in the chat box
3. **Get Answer**: The system will automatically retrieve relevant documents and generate answers

### API Usage

#### 1. Retrieve Relevant Documents

```bash
curl -X POST "http://localhost:8010/retrieveDocs" \
  -H "Content-Type: application/json" \
  -d '{"message": "å¦‚ä½•é€²è¡Œç·šä¸Šé–‹æˆ¶ï¼Ÿ"}'
```

**Response Example:**
```json
{
  "response": "[{'rank': 1, 'doc_id': 1, 'score': 0.85, 'topic': 'é–‹æˆ¶æµç¨‹', ...}]",
  "status": "success"
}
```

#### 2. Get Intelligent Answer

```bash
curl -X POST "http://localhost:8010/response" \
  -H "Content-Type: application/json" \
  -d '{"message": "å¦‚ä½•é€²è¡Œç·šä¸Šé–‹æˆ¶ï¼Ÿ"}'
```

**Response Example:**
```json
{
  "response": "ç·šä¸Šé–‹æˆ¶æµç¨‹å¦‚ä¸‹ï¼š\n1. æº–å‚™èº«åˆ†è­‰èˆ‡...",
  "status": "success"
}
```

---

## ğŸ“‚ Project Structure

```
salesFAQBot/
â”‚
â”œâ”€â”€ config/                          # Configuration files
â”‚   â””â”€â”€ config.yaml                  # Main system configuration
â”‚
â”œâ”€â”€ core/                            # Core functionality modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ init/                        # Initialization module
â”‚   â”‚   â”œâ”€â”€ builder.py               # System initialization builder
â”‚   â”‚   â””â”€â”€ database.py              # Qdrant database initialization
â”‚   â”‚
â”‚   â”œâ”€â”€ embedder/                    # Embedding module
â”‚   â”‚   â”œâ”€â”€ base_embedder.py         # Embedder base class (abstract)
â”‚   â”‚   â”œâ”€â”€ sentence_transformer_embedder.py  # ST embedder implementation
â”‚   â”‚   â””â”€â”€ bm25.py                  # BM25 sparse embedder
â”‚   â”‚
â”‚   â”œâ”€â”€ reranker/                    # Reranking module
â”‚   â”‚   â”œâ”€â”€ base_reranker.py         # Reranker base class (abstract)
â”‚   â”‚   â””â”€â”€ sentence_transformer_reranker.py  # ST reranker implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieve/                    # Retrieval module
â”‚   â”‚   â”œâ”€â”€ dense_search.py          # Dense vector retrieval
â”‚   â”‚   â”œâ”€â”€ bm25_search.py           # BM25 sparse retrieval
â”‚   â”‚   â””â”€â”€ rerank_search.py         # Reranking retrieval
â”‚   â”‚
â”‚   â””â”€â”€ response/                    # Response generation module
â”‚       â”œâ”€â”€ geminiAPI.py             # Gemini API integration
â”‚       â”œâ”€â”€ aoai.py                  # Azure OpenAI API integration
â”‚       â””â”€â”€ prompt.py                # Prompt constructor
â”‚
â”œâ”€â”€ db/                              # Vector database (auto-generated)
â”‚   â”œâ”€â”€ meta.json                    # Qdrant metadata
â”‚   â””â”€â”€ collection/                  # Vector collection storage
â”‚       â””â”€â”€ FAQ/                     # FAQ collection data
â”‚
â”œâ”€â”€ knowledgeDoc/                    # Knowledge base source files
â”‚   â””â”€â”€ *.xlsx                       # FAQ data in Excel format
â”‚
â”œâ”€â”€ logs/                            # Log files (auto-generated)
â”‚   â””â”€â”€ app.log                      # Application logs
â”‚
â”œâ”€â”€ models/                          # AI model files (auto-downloaded)
â”‚   â”œâ”€â”€ bge-m3/                      # BGE-M3 embedding model
â”‚   â”œâ”€â”€ jina-reranker-v2-base-multilingual/  # Jina reranker
â”‚   â””â”€â”€ ...                          # Other models
â”‚
â”œâ”€â”€ static/                          # Static resources
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css                # Web styles
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ script.js                # Frontend interaction logic
â”‚
â”œâ”€â”€ template/                        # HTML templates
â”‚   â””â”€â”€ index.html                   # Main chat interface
â”‚
â”œâ”€â”€ test/                            # Test scripts
â”‚   â”œâ”€â”€ test_bm25.py                 # BM25 tests
â”‚   â””â”€â”€ test_reranker.py             # Reranker tests
â”‚
â”œâ”€â”€ test_records/                    # Test records
â”‚
â”œâ”€â”€ utils/                           # Utility functions
â”‚   â””â”€â”€ corpus.py                    # Text processing utilities
â”‚
â”œâ”€â”€ .env                             # Environment variables (create manually)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ main.py                          # FastAPI main program
â””â”€â”€ README.md                        # Project documentation
```

### Core Module Descriptions

#### `core/init/`
- **builder.py**: Handles initialization flow during system startup, including loading knowledge base, creating embedding vectors, and initializing database
- **database.py**: Encapsulates Qdrant database creation and configuration logic

#### `core/embedder/`
- **base_embedder.py**: Defines the abstract interface for embedders
- **sentence_transformer_embedder.py**: Implementation based on Sentence Transformers
- **bm25.py**: Implements BM25 sparse embedding

#### `core/retrieve/`
- **dense_search.py**: Semantic vector retrieval (primary method)
- **bm25_search.py**: Keyword retrieval
- **rerank_search.py**: Retrieval result reranking

#### `core/response/`
- **geminiAPI.py**: Interacts with Google Gemini API to generate final answers
- **aoai.py**: Interacts with Azure OpenAI API to generate final answers
- **prompt.py**: Constructs RAG prompt templates using class-based approach with cached properties

---

## âš™ï¸ Configuration

### config.yaml Detailed Description

```yaml
# FastAPI server settings
server:
  cors_origins: ["*"]          # CORS allowed origins (restrict in production)
  max_message_length: 1024     # Maximum message length
  cors_max_age: 3600           # CORS preflight request cache time (seconds)

# Vector database settings
db:
  collection_name: "FAQ"       # Qdrant collection name

# Embedding model settings
embedder:
  type: "sentence_transformer"            # Embedder type
  repo: "BAAI/bge-m3"                    # HuggingFace model repository

# Retrieval settings
retriever:
  top_k: 3                     # Number of documents to retrieve
  score_threshold: 0.5         # Minimum similarity score threshold

# Reranking settings (optional)
reranker:
  apply: False                 # Whether to enable reranking
  type: "sentence_transformer" # Reranker type
  repo: "jinaai/jina-reranker-v2-base-multilingual"
  top_k: 3                     # Number to keep after reranking
  score_threshold: 0.5         # Reranking score threshold
```

---

## ğŸ”§ Development

### Using Different LLM Providers

The system now supports multiple LLM providers:

1. **Google Gemini API** (via [geminiAPI.py](core/response/geminiAPI.py))
2. **Azure OpenAI API** (via [aoai.py](core/response/aoai.py))

Both use the unified `prompt_template` class from [prompt.py](core/response/prompt.py):

```python
from core.response.prompt import prompt_template

# System prompt is cached for efficiency using @cached_property
system_prompt = prompt_template().system_prompt

# Construct user prompt with retrieved documents
user_prompt = prompt_template.construct(query, top_k_docs)
```

### Extending Embedding Models

1. Inherit from `BaseEmbedder` base class:

```python
# core/embedder/my_embedder.py
from core.embedder.base_embedder import BaseEmbedder

class MyEmbedder(BaseEmbedder):
    def __init__(self, config):
        # Initialization logic
        pass
    
    def encode(self, texts, encode_type):
        # Embedding logic
        return embeddings
```

2. Configure in `config.yaml`:

```yaml
embedder:
  type: "my_embedder"
  # Other configurations...
```

### Extending Retrieval Strategies

Add new retrieval classes in the `core/retrieve/` directory:

```python
# core/retrieve/my_search.py
class MySearcher:
    def search(self, query, k=3, score=0.4):
        # Retrieval logic
        return results
```

### Log Management

Logs are output to:
- **Console**: Real-time viewing
- **File**: `logs/app.log`

Log level: INFO (can be adjusted in [main.py](main.py))

---

## ğŸ§ª Testing

### Running Tests

```powershell
# Test BM25 retrieval
uv run test\test_bm25.py

# Test reranker
uv run test\test_reranker.py

# Test API responses
uv run test\test_response.py
```
---

## ğŸ“ Contact

For any questions or suggestions, please contact:

**Project Team**
- **Email**: tom.h.huang@fubon.com, kris.yj.chen@fubon.com
- **Phone**: 02-87716888 #69175, 02-66080879 #69194
- **Department**: Securities Data Science Department, Data Service Division (5F)

**Issue Reporting**
- Please submit bugs or feature requests via GitHub Issues
- Pull Request contributions are welcome

---

## ğŸ“„ License

This project is for internal use only. Copyright belongs to Fubon Securities Data Science Department.

<!-- Markdown link & img dfn's -->

[python-image]: https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54
[fastapi-image]: https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi
[Qdrant-image]: https://img.shields.io/badge/Qdrant-Vector%20DB-FF6B6B?style=for-the-badge
[HuggingFace-image]: https://img.shields.io/badge/-HuggingFace-3B4252?style=flat&logo=huggingface&logoColor=
[Gemini-image]: https://img.shields.io/badge/google%20gemini-8E75B2?style=for-the-badge&logo=google%20gemini&logoColor=white
[HTML-image]: https://img.shields.io/badge/html-%23E34F26?style=for-the-badge&logo=html5&logoColor=%23fff
